{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Web-scraping Auckland CBD Business Directory</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Import Dependencies</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install w3lib\n",
    "!pip install selenium\n",
    "!pip install geocoder\n",
    "!pip install geopy\n",
    "!pip install geopy\n",
    "!pip install folium\n",
    "\n",
    "import numpy as np  # library to handle data in a vectorized manner\n",
    "import pandas as pd  # library for data analsysis\n",
    "import requests\n",
    "import csv\n",
    "import pprint as pp\n",
    "import time\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "from w3lib.url import url_query_parameter\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# from selenium.common.exceptions import IndexError\n",
    "\n",
    "from bs4 import BeautifulSoup as bs  # Beautiful Soup v4\n",
    "\n",
    "# Only use when full dataframe needs to be viewed\n",
    "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "import geocoder  # import geocoder\n",
    "\n",
    "# module to convert an address into latitude and longitude values\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# libraries for displaying images\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# tranforming json file into a pandas dataframe library\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import folium  # plotting library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Define Directory Start Page</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'DIRECTORY START PAGE'  # Typically where the base information about the directory resides\n",
    "html = requests.get(url).text\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Set Up Web Control</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.page_load_strategy = 'eager'\n",
    "# Add options you desire, e.g. headless\n",
    "\n",
    "driver = wd.Chrome(\n",
    "    'FULL DRIVER EXECUTABLE PATH\\chromedriver.exe', options=options)\n",
    "\n",
    "# View target url to test functionality of driver (won't work in headless mode)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Get Total Count of Directory Items</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = soup.find(TARGET THE DIRECTORY COUNT IN RELEVANT ELEMENT)\n",
    "count = count.string      # convert it to string,\n",
    "k = int(count)            # then integer,\n",
    "print(k)                  # and print it to confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Create Lists for Data Storage</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_l = []\n",
    "main_category_l = []\n",
    "sub_category1_l = []\n",
    "sub_category2_l = []\n",
    "str_addy_pre_l = []\n",
    "str_addy_l = []\n",
    "precinct_l = []\n",
    "postcode_l = []\n",
    "locality_l = []\n",
    "phone_l = []\n",
    "website_l = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Set Up and Run Data Extraction Loop</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0             # Item index per page\n",
    "d = 0             # Page index\n",
    "e = b + (d * ITEMS PER PAGE)  # Total items accessed\n",
    "\n",
    "for e in range(0, k):\n",
    "    try:\n",
    "        container = soup.find_all(GROUP ELEMENT CONTAINING DIRECTORY INFO)[b]\n",
    "        \n",
    "        name = container.splitlines()[LINE NUMBER CONTAINING NAME]           # Always required to exist in the directory\n",
    "        try:\n",
    "            main_category = container.splitlines()[LINE NUMBER CONTAINING MAIN CATEGORY]\n",
    "        except:\n",
    "            main_category = 'Uncategorized'\n",
    "        try:\n",
    "            sub_category1 = container.splitlines()[LINE NUMBER CONTAINING SUB CATEGORY 1]\n",
    "        except:\n",
    "            sub_category1 = ''\n",
    "        try:\n",
    "            sub_category2 = container.splitlines()[LINE NUMBER CONTAINING SUB CATEGORY 2]\n",
    "        except:\n",
    "            sub_category2 = ''\n",
    "        try:\n",
    "            str_addy_pre = container.splitlines()[LINE NUMBER CONTAINING ADDRESS FIELD 1]\n",
    "        except:\n",
    "            str_addy_pre = ''\n",
    "        try:\n",
    "            str_addy = container.splitlines()[LINE NUMBER CONTAINING ADDRESS FIELD 2]\n",
    "        except:\n",
    "            str_addy = ''\n",
    "            \n",
    "        precinct = container.splitlines()[LINE NUMBER CONTAINING PRECINCT]   # Always added by directory administrator\n",
    "        postcode = container.splitlines()[LINE NUMBER CONTAINING POSTCODE]   # Always added by directory administrator\n",
    "        locality = container.splitlines()[LINE NUMBER CONTAINING PRECINCT]   # Always added by directory administrator\n",
    "\n",
    "        try:\n",
    "            phone = container.splitlines()[LINE NUMBER CONTAINING PHONE NUMBER]\n",
    "        except:\n",
    "            phone = ''        \n",
    "        try:\n",
    "            website = container.splitlines()[LINE NUMBER CONTAINING WEBSITE URL]\n",
    "        except:\n",
    "            website = ''\n",
    "\n",
    "    except IndexError:                    # An index error indicates end of page\n",
    "        b = 0                             # Reset item index per page\n",
    "        \n",
    "        # Use Selenium to control current webpage by accessing the next page\n",
    "        clicker = driver.FIND ELEMENT TO ACCESS NEXT PAGE\n",
    "        clicker.click()                   # Execute click    \n",
    "        current_url = driver.current_url  # get the current url\n",
    "        print(current_url)                # print the current url to confirm\n",
    "        d = SERIAL PAGE NUMBER FROM CURRENT URL\n",
    "        d = int(d)\n",
    "\n",
    "        html = requests.get(current_url).text\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        container = soup.find_all(GROUP ELEMENT CONTAINING DIRECTORY INFO)[b]\n",
    "        \n",
    "        name = container.splitlines()[LINE NUMBER CONTAINING NAME]           # Always required to exist in the directory\n",
    "        try:\n",
    "            main_category = container.splitlines()[LINE NUMBER CONTAINING MAIN CATEGORY]\n",
    "        except:\n",
    "            main_category = 'Uncategorized'\n",
    "        try:\n",
    "            sub_category1 = container.splitlines()[LINE NUMBER CONTAINING SUB CATEGORY 1]\n",
    "        except:\n",
    "            sub_category1 = ''\n",
    "        try:\n",
    "            sub_category2 = container.splitlines()[LINE NUMBER CONTAINING SUB CATEGORY 2]\n",
    "        except:\n",
    "            sub_category2 = ''\n",
    "        try:\n",
    "            str_addy_pre = container.splitlines()[LINE NUMBER CONTAINING ADDRESS FIELD 1]\n",
    "        except:\n",
    "            str_addy_pre = ''\n",
    "        try:\n",
    "            str_addy = container.splitlines()[LINE NUMBER CONTAINING ADDRESS FIELD 2]\n",
    "        except:\n",
    "            str_addy = ''\n",
    "            \n",
    "        precinct = container.splitlines()[LINE NUMBER CONTAINING PRECINCT]   # Always added by directory administrator\n",
    "        postcode = container.splitlines()[LINE NUMBER CONTAINING POSTCODE]   # Always added by directory administrator\n",
    "        locality = container.splitlines()[LINE NUMBER CONTAINING PRECINCT]   # Always added by directory administrator\n",
    "\n",
    "        try:\n",
    "            phone = container.splitlines()[LINE NUMBER CONTAINING PHONE NUMBER]\n",
    "        except:\n",
    "            phone = ''        \n",
    "        try:\n",
    "            website = container.splitlines()[LINE NUMBER CONTAINING WEBSITE URL]\n",
    "        except:\n",
    "            website = ''\n",
    "\n",
    "    name_l.append(name)\n",
    "    main_category_l.append(main_category)\n",
    "    sub_category1_l.append(sub_category1)\n",
    "    sub_category2_l.append(sub_category2)\n",
    "    str_addy_pre_l.append(str_addy_pre)\n",
    "    str_addy_l.append(str_addy)\n",
    "    precinct_l.append(precinct)\n",
    "    postcode_l.append(postcode)\n",
    "    locality_l.append(locality)\n",
    "    phone_l.append(phone)\n",
    "    website_l.append(website)\n",
    "    \n",
    "    # Increment indices\n",
    "    b += 1\n",
    "    e = b + (d * ITEMS PER PAGE)  \n",
    "    print(e)     # To keep track of count\n",
    "\n",
    "driver.quit()\n",
    "print(\"Web Scrapping done.......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Create, Populate and View DataFrame</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Name': name_l,\n",
    " 'Main Category': main_category_l,\n",
    " 'Sub Category 1': sub_category1_l,\n",
    " 'Sub Category 2': sub_category2_l,\n",
    " 'Address Line 1': str_addy_pre_l,\n",
    " 'Address Line 2': str_addy_l,\n",
    " 'Precinct': precinct_l,\n",
    " 'Post Code': postcode_l,\n",
    " 'Locality': locality_l,\n",
    " 'Phone': phone_l,\n",
    " 'Web URL': website_l\n",
    "}\n",
    "\n",
    "akl_cbd_biz_extract = pd.DataFrame(data)\n",
    "akl_cbd_biz_extract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Save DataFrame as Excel File</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akl_cbd_biz_extract.to_excel(\n",
    "    r'FULL FILE PATH\\FILE.xlsx', index=False, header=True)  # Saved as \"web_scraper.xlsx\"\n",
    "\n",
    "print(\"Saved to excel file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
